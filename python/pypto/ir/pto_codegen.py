# Copyright (c) PyPTO Contributors.
# This program is free software, you can redistribute it and/or modify it under the terms and conditions of
# CANN Open Software License Agreement Version 2.0 (the "License").
# Please refer to the License for details. You may not use this file except in compliance with the License.
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
# INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
# See LICENSE in the root of the software repository for the full text of the License.
# -----------------------------------------------------------------------------------------------------------

"""PTO backend code generation (Python side).

Generates all output files for the PTO backend, analogous to C++ CCECodegen:

- **Kernel files**: InCore functions go through PTOCodegen → ptoas → kernel wrapper
- **Orchestration**: Reuses the shared C++ orchestration codegen (PTO2 runtime API)
- **Config**: Generates kernel_config.py with runtime/orchestration/kernel metadata

Entry point: ``generate(program, output_dir) -> dict[str, str]``
"""

import os
import re
import shutil
import subprocess

from pypto.pypto_core import codegen as _codegen_core
from pypto.pypto_core import ir as _ir_core

_PTOAS_RELEASE_URL = "https://github.com/zhangstevenunity/PTOAS/releases"


def _run_ptoas(
    pto_path: str,
    output_path: str,
    ptoas_flags: list[str] | None = None,
) -> None:
    """Run the ptoas tool to compile a .pto file to C++.

    Locates ptoas via PTOAS_ROOT env var (``$PTOAS_ROOT/ptoas``) or PATH fallback.

    Args:
        pto_path: Path to the input .pto file
        output_path: Path for the output .cpp file
        ptoas_flags: Additional flags to pass to ptoas (optional)

    Raises:
        FileNotFoundError: If the ptoas binary cannot be found
        RuntimeError: If ptoas compilation fails
    """
    ptoas_root = os.environ.get("PTOAS_ROOT")
    if ptoas_root:
        ptoas_bin = os.path.join(ptoas_root, "ptoas")
        if not (os.path.isfile(ptoas_bin) and os.access(ptoas_bin, os.X_OK)):
            raise FileNotFoundError(
                f"PTOAS_ROOT is set to '{ptoas_root}' but '{ptoas_bin}' does not exist or is not executable. "
            )
    else:
        ptoas_bin = shutil.which("ptoas")
        if not ptoas_bin:
            raise FileNotFoundError(
                "ptoas binary not found. Set PTOAS_ROOT to the extracted release directory, "
                f"or add ptoas to your PATH.\nDownload from: {_PTOAS_RELEASE_URL}"
            )

    cmd = [ptoas_bin, pto_path, "-o", output_path]
    if ptoas_flags:
        cmd.extend(ptoas_flags)

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            timeout=60,
        )
    except subprocess.TimeoutExpired as exc:
        raise RuntimeError(f"ptoas compilation timed out after {exc.timeout}s") from exc
    if result.returncode != 0:
        raise RuntimeError(f"ptoas compilation failed: {result.stderr.strip()}")


_KERNEL_HEADER = """\
// Kernel Function: {func_name}
// Generated by PyPTO IR Compiler (PTO backend)

#include <cstdint>
#include <pto/pto-inst.hpp>
#include "tensor.h"

using namespace pto;

#ifndef __gm__
#define __gm__
#endif

#ifndef __aicore__
#define __aicore__ [aicore]
#endif

// Helper function to compute stride from raw_shapes
__aicore__ __attribute__((always_inline)) inline int64_t compute_stride(
    __gm__ TensorData* tensor, int dim) {{
  int64_t stride = 1;
  for (int j = dim + 1; j < static_cast<int>(tensor->ndims); j++) {{
    stride *= static_cast<int64_t>(tensor->raw_shapes[j]);
  }}
  return stride;
}}
"""


def _preprocess_ptoas_output(content: str) -> str:
    """Strip includes/using and make functions static in ptoas output.

    Removes the header lines that the wrapper already provides, and replaces
    ``__global__ AICORE void`` with ``static __aicore__ void`` so the wrapper's
    ``kernel_entry`` is the actual entry point.
    """
    lines = content.splitlines(keepends=True)
    filtered: list[str] = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith("#include") and (
            "pto-inst" in stripped or "cstdint" in stripped or "tensor.h" in stripped
        ):
            continue
        if stripped == "using namespace pto;":
            continue
        filtered.append(line)
    result = "".join(filtered)
    result = re.sub(r"__global__\s+AICORE\s+void", "static __aicore__ void", result)
    return result


def _generate_arg_unpacking(func: _ir_core.Function) -> tuple[str, list[str]]:
    """Generate C++ code to unpack ``int64_t* args`` into typed locals.

    Returns:
        A tuple of (C++ unpacking code, list of local variable names in order).
    """
    lines: list[str] = []
    var_names: list[str] = []

    for i, param in enumerate(func.params):
        param_name = param.name
        param_type = param.type

        if isinstance(param_type, _ir_core.TensorType):
            c_type = param_type.dtype.to_c_type_string()
            lines.append(f"    // Unpack tensor: {param_name}")
            lines.append(
                f"    __gm__ TensorData* {param_name}_tensor = "
                f"reinterpret_cast<__gm__ TensorData*>(args[{i}]);"
            )
            lines.append(
                f"    __gm__ {c_type}* {param_name} = "
                f"reinterpret_cast<__gm__ {c_type}*>("
                f"{param_name}_tensor->buffer.addr) + {param_name}_tensor->start_offset;"
            )
            var_names.append(param_name)

        elif isinstance(param_type, _ir_core.ScalarType):
            c_type = param_type.dtype.to_c_type_string()
            lines.append(f"    // Unpack scalar: {param_name}")
            lines.append(f"    union {{ uint64_t u64; {c_type} val; }} {param_name}_conv;")
            lines.append(f"    {param_name}_conv.u64 = args[{i}];")
            lines.append(f"    {c_type} {param_name} = {param_name}_conv.val;")
            var_names.append(param_name)

        else:
            raise ValueError(
                f"Unsupported parameter type for wrapper generation: "
                f"{type(param_type).__name__} in function {func.name}"
            )

        lines.append("")  # blank line between params

    return "\n".join(lines), var_names


def _generate_kernel_wrapper(func: _ir_core.Function, ptoas_code: str) -> str:
    """Generate a complete CCE-compatible wrapper file for one InCore function.

    Combines:
    1. CCE kernel header (includes, macros)
    2. Preprocessed ptoas code (static, no duplicate includes)
    3. ``kernel_entry`` wrapper with arg unpacking and forward call
    """
    header = _KERNEL_HEADER.format(func_name=func.name)
    ptoas_body = _preprocess_ptoas_output(ptoas_code)
    unpacking_code, var_names = _generate_arg_unpacking(func)
    call_args = ", ".join(var_names)

    wrapper_func = (
        "// --- CCE-compatible entry point ---\n"
        'extern "C" __aicore__ __attribute__((always_inline)) '
        "void kernel_entry(__gm__ int64_t* args)\n"
        "{\n"
        f"{unpacking_code}\n"
        f"    // Forward to ptoas-generated function\n"
        f"    {func.name}({call_args});\n"
        "}\n"
    )

    return f"{header}\n// --- ptoas-generated code ---\n{ptoas_body}\n{wrapper_func}"


def _generate_config_file(
    orch_func_name: str,
    func_name_to_id: dict[str, int],
    func_name_to_core_type: dict[str, _ir_core.CoreType],
) -> str:
    """Generate kernel_config.py content matching the CCE format."""
    lines = [
        "# Kernel and Orchestration Configuration\n",
        "from pathlib import Path\n",
        "_ROOT_DIR = Path(__file__).parent\n",
        "# Runtime configuration for tensormap_and_ringbuffer",
        "# This runtime requires 4 AICPU threads (3 schedulers + 1 orchestrator on thread 3)",
        "RUNTIME_CONFIG = {",
        '\t"runtime": "tensormap_and_ringbuffer",',
        '\t"aicpu_thread_num": 4,',
        '\t"block_dim": 24,',
        "}\n",
        "ORCHESTRATION = {",
        f'\t"source": str(_ROOT_DIR / "orchestration" / "{orch_func_name}.cpp"),',
        '\t"function_name": "aicpu_orchestration_entry"',
        "}\n",
        "KERNELS = [",
    ]

    for name, func_id in sorted(func_name_to_id.items(), key=lambda x: x[1]):
        core_type = func_name_to_core_type[name]
        ct_str = "aiv" if core_type == _ir_core.CoreType.VECTOR else "aic"
        lines.append(
            f'\t{{"func_id": {func_id}, '
            f'"source": str(_ROOT_DIR / "kernels" / "{ct_str}" / "{name}.cpp"), '
            f'"core_type": "{ct_str}"}},'
        )

    lines.append("]")
    return "\n".join(lines) + "\n"


def generate(
    transformed_program: _ir_core.Program,
    output_dir: str,
    skip_ptoas: bool = False,
) -> dict[str, str]:
    """Generate all PTO backend output files (kernels + orchestration + config).

    Analogous to ``CCECodegen.generate()`` — returns a complete file map for the
    PTO backend. Kernel InCore functions go through the ptoas pipeline by default;
    when ``skip_ptoas=True``, the raw MLIR (.pto) content is returned directly
    without invoking ptoas.

    Args:
        transformed_program: Program after pass pipeline
        output_dir: Base output directory (used for ptoas intermediates when skip_ptoas=False)
        skip_ptoas: When True, skip the ptoas compilation step and return raw MLIR
            content in result_files with .pto extension instead of compiled .cpp wrappers.

    Returns:
        Dict mapping relative file paths to their content.
    """
    result_files: dict[str, str] = {}
    orch_func: _ir_core.Function | None = None

    for func in transformed_program.functions.values():
        if func.func_type == _ir_core.FunctionType.Orchestration:
            orch_func = func
            continue
        if func.func_type != _ir_core.FunctionType.InCore:
            continue

        single_program = _ir_core.Program([func], func.name, transformed_program.span)
        codegen_instance = _codegen_core.PTOCodegen()
        pto_code = codegen_instance.generate(single_program)
        core_type = _codegen_core.infer_function_core_type(func)
        ct_str = "aiv" if core_type == _ir_core.CoreType.VECTOR else "aic"

        if skip_ptoas:
            # Return raw MLIR content directly without invoking ptoas
            kernel_rel = os.path.join("kernels", ct_str, f"{func.name}.pto")
            result_files[kernel_rel] = pto_code
        else:
            # Per-InCore: PTOCodegen → .pto → ptoas → .cpp → wrapper
            ptoas_dir = os.path.join(output_dir, "ptoas")
            os.makedirs(ptoas_dir, exist_ok=True)

            pto_path = os.path.join(ptoas_dir, f"{func.name}.pto")
            with open(pto_path, "w") as f:
                f.write(pto_code)

            cpp_path = os.path.join(ptoas_dir, f"{func.name}.cpp")
            _run_ptoas(pto_path, cpp_path, ptoas_flags=["--enable-insert-sync"])

            with open(cpp_path) as f:
                ptoas_cpp = f.read()

            wrapper_code = _generate_kernel_wrapper(func, ptoas_cpp)
            kernel_rel = os.path.join("kernels", ct_str, f"{func.name}.cpp")
            result_files[kernel_rel] = wrapper_code

    # Orchestration + config (shared codegen with CCE)
    if orch_func is not None:
        orch_result = _codegen_core.generate_orchestration(transformed_program, orch_func)
        result_files[f"orchestration/{orch_func.name}.cpp"] = (
            f"// Orchestration Function: {orch_func.name}\n"
            f"// Generated by PyPTO IR Compiler\n\n"
            f"{orch_result.code}"
        )
        if not skip_ptoas:
            result_files["kernel_config.py"] = _generate_config_file(
                orch_func.name,
                orch_result.func_name_to_id,
                orch_result.func_name_to_core_type,
            )

    return result_files
